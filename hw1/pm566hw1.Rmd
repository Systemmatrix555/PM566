---
title: "PM 566 Assignment 1"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Chris Hanson
## September 24, 2021
#  
### In this assignment I am given the task of determining whether daily concentration of PM2.5 has decreased in California in the last 15 years. To do this I am using data provided by the U.S. EPA.
<br>
<br>
```{r, message = FALSE}
rm(list = ls())

# Load libraries
library(tidyverse)
library(data.table)
library(janitor)
library(leaflet)
library(ggplot2)
library(lubridate)

```


### **Problem 1.** _Read in the data. Check dimensions, headers, footers, variable names, and variable types. Check for any data issues, particularly in the key variables. Summarize all findings._
```{r load and check data}
# Read in the data using data.table().
pm2004 <- data.table::fread("ad_viz_plotval_data_2004.csv")
pm2004 <- clean_names(pm2004)
pm2019 <- data.table::fread("ad_viz_plotval_data_2019.csv")
pm2019 <- clean_names(pm2019)
# Check dimensions
paste("The 2004 dataset has", dim(pm2004)[1], "observations and", dim(pm2004)[2], "variables.")
paste("The 2019 dataset has", dim(pm2019)[1], "observations and", dim(pm2019)[2], "variables.")

# Check headers
head(pm2004)
head(pm2019)

# Check footers
tail(pm2004)
tail(pm2019)

# Check variable names
paste(colnames(pm2004))

# Check variable types
sapply(pm2004, typeof)

# Check for issues in key variables:
str(pm2004) # General readout
summary(pm2004) #Summary statistics
summary(pm2004$daily_mean_pm2_5_concentration) #Key variable
mean(is.na(pm2004$daily_mean_pm2_5_concentration)) #No missing values
hist(pm2004$daily_mean_pm2_5_concentration, breaks = 100, xlim=c(0,100)) #Check for outliers

str(pm2019) # General readout
summary(pm2019) #Summary statistics
summary(pm2019$daily_mean_pm2_5_concentration) #Key variable
mean(is.na(pm2019$daily_mean_pm2_5_concentration)) #No missing values
hist(pm2019$daily_mean_pm2_5_concentration, breaks = 100, xlim=c(0,100)) #Check for outliers

```

Summary of findings: 
While the Max values of PM 2.5 in each of these data sets is extremely high, they don't seem to be errors - just extreme events.


### **Problem 2.** _Combine the two years of data into one data frame. Use the Date variable to create a new column for year, which will serve as an identifier. Change the names of the key variables so that they are easier to refer to in your code._

```{r combine and clean data}
# Combine the two years of data into one data frame.
pm <- rbind(pm2004, pm2019)

# Use the Date variable to create a new column for year, which will serve as an identifier.
dates <- pm$date
dates2 <- as.POSIXct(dates, format = "%m/%d/%Y")
dates3 <- format(dates2, format="%Y") 

pm <-  mutate(pm, year = dates3) #New column "year" added

# Change the names of the key variables so that they are easier to refer to in your code.
pm <- rename(pm, pm25 = daily_mean_pm2_5_concentration)

pm$year <- as.numeric(pm$year) #so that colorNumeric works, but can instead use colorFactor

```

### **Problem 3.** _Create a basic map in leaflet() that shows the locations of the sites (make sure to use different colors for each year). Summarize the spatial distribution of the monitoring sites._

```{r}
# Create a basic map in leaflet() that shows the locations of the sites (make sure to use different colors for each year).
pal <- colorNumeric(palette = "RdYlBu", domain=c(2004, 2019)) 
#colorNumeric doesn't work any more

leaflet(pm) %>%
  addProviderTiles('OpenStreetMap') %>% 
  addCircles(lat=~site_latitude,lng=~site_longitude, color=pal(pm$year), opacity=1, fillOpacity=1, radius=100)
  #addLegend('bottomleft', pal=pal, values=~c(2004, 2019),
          #title='Site Locations', opacity=1)
    #this legend is driving me crazy

```

Summarize the spatial distribution of the monitoring sites: ???

### **Problem 4.** _Check for any missing or implausible values of PM2.5 in the combined data set. Explore the proportions of each and provide a summary of any temporal patterns you see in these observations._

```{r}
# Check for any missing or implausible values of PM2.5 in the combined data set.
mean(is.na(pm))
mean(is.na(pm$cbsa_code))
mean(is.na(pm$cbsa_code))/mean(is.na(pm))
# 0.36% of all data is missing, and 7.5% of the "cbsa_code" data is missing. As 7.5 is 21x greater than 0.36, and there are 21 variables, that means all missing data is from the "cbsa_code" column.
# ---------------------------------------------------------------------------
# Extreme values
boxplot(pm$daily_aqi_value)
summary(pm$daily_aqi_value)
#Here we see an extreme outlier of AQI = 301. According to airnow.gov, an AQI (air quality index) of 301 or higher represents an emergency condition. This value is most associated with wildfires, which is not unreasonable.

boxplot(pm$pm25)
summary(pm$pm25)
# PM 2.5 has an extremely high Max value of 251, perhaps this is associated with the same wildfire event observed in the high AQI event discovered above:
pm[which(grepl(251, pm$pm25)), daily_aqi_value]
#The extreme PM2.5 and AQI values did indeed come from the same reading, suggesting they are both accurate readings of an extreme weather event.

# at which site is this supposed wildfire happening?
wildfire_site_index = which(grepl(251, pm$pm25))
wildfire_site_id = pm[wildfire_site_index,site_id]
#what year?
wildfire_year = pm[wildfire_site_index,year]
#let's plot PM2.5 at that site to get the story
pmfire <- filter(pm, site_id == wildfire_site_id, year == wildfire_year)
ggplot(data = pmfire, aes(x=date, y=pm25)) +
  geom_point()
# that graph pretty clearly tells the story of a wildfire.

ggplot(data = pmfire, aes(x=date, y=daily_aqi_value)) +
  geom_point()
#the daily aqi value confirms the story.


## Let's see if the sites with the highest average PM2.5 sites seem plausible...
# calc ave PM2.5 by site
pm_ave <- pm %>% group_by(site_id) %>% summarize(mean25 = mean(pm25))
# find the highest average pm2.5 value
max(pm_ave$mean25)
#which index of pm_ave has this max average value?
high_site_index = which(pm_ave$mean25 %in% max(pm_ave$mean25))
#what is the site id associated with this max value?
high_site = pm_ave$site_id[high_site_index]
#let's graph pm25 at this site in 2004
pmhigh_2004 <- filter(pm, site_id == high_site, year == 2004)
ggplot(data = pmhigh_2004, aes(x=date, y=pm25)) +
  geom_point()
#I guess this wasn't very revealing, let's try the second highest value.
next_highest = max( pm_ave$mean25[pm_ave$mean25!=max(pm_ave$mean25)])
nexthighest_index = which(pm_ave$mean25 %in% next_highest)
nexthighest_site = pm_ave$site_id[nexthighest_index]
pmnexthigh_2004 <- filter(pm, site_id == nexthighest_site, year == 2004)
ggplot(data = pmnexthigh_2004, aes(x=date, y=pm25)) +
  geom_point()
# This was done to reveal if there were temporal patterns in the data set. While these high points are extreme, they are not unbelievable - but they don't suggest a wildfire like the previous graphs do.
```

### **Problem 5.** _Explore the main question of interest at three different spatial levels. Create exploratory plots (e.g. boxplots, histograms, line plots) and summary statistics that best suit each level of data. Be sure to write up explanations of what you observe in these data._
_State_
_County_
_Site in Los Angeles_

```{r}
# (much of this code was inspired by Chapter 16 of Exploratory Data Analysis by Roger Peng)

##### State

#Boxplots of 2004 and 2019 PM25 for the entire state
ggplot(data = pm, aes(y=pm25)) +
  geom_boxplot()+
  facet_wrap(~year)+
  coord_cartesian(ylim = c(0,150))

#Summary of 2004 and 2019 PM2.5 statistics for the entire state
with(pm, tapply(pm25, year, summary))

#It is clear from these box plots that 2019 has a lower mean, smaller spread, and fewer extreme outliers in PM2.5 than did 2004.


#Investigate negative values
filter(pm, year == "2004") %>% summarize(negative = mean(pm25 < 0, na.rm = TRUE))
#A minuscule amount of the data is negative.

##### County

#Filtering down the entire data set to just one county
LA25 <- filter(pm, county == "Los Angeles")

#Box plot of PM2.5 in only Los Angeles County
ggplot(data = LA25, aes(y=pm25)) +
  geom_boxplot()+
  facet_wrap(~year)

with(LA25, tapply(pm25, year, summary))

##### Site in Los Angeles county
#Finding an interesting site
unique(sites$site_name)

#Finding a site with data in both years:
##List of all unique site IDs in LA county
sites <- filter(pm, county == "Los Angeles") %>% select(site_id, year) %>% unique
site.year <- with(sites, split(site_id, year))
both <- intersect(site.year[[1]], site.year[[2]])
#Make a data frame with only sites from LA county in use in both years
count <- mutate(pm) %>% filter(site_id %in% both)
#Get a count of readings at each site to ensure the one we choose is interesting
group_by(count, site_id) %>% summarize(n = n())
#Let's use site_id 60371103 (North Main Street!)
mainst25 <- filter(pm, site_id == 60371103) %>% select(date, year, pm25) %>% mutate(date2 = as.Date(date, format="%m/%d/%Y"), yday = yday(date2))

qplot(yday, pm25, data=mainst25, facets = . ~ year, xlab = "Day of the year")
```

