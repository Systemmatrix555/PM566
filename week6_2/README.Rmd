---
title: "pm566week6lab"
author: "Chris Hanson"
date: "10/1/2021"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
```

```{r libraries, cache = TRUE}
library(tidyverse)
library(tidytext)
library(ggplot2)
```

## Data set: Descriptions of medical appointments, the medical specialty, a "transcription" of the outcome, and keywords.

```{r load-data, cache = TRUE}
fn  <- "mtsamples.csv"

if (!file.exists(fn))
  download.file("https://raw.githubusercontent.com/USCbiostats/data-science-data/master/00_mtsamples/mtsamples.csv", destfile = fn)

mtsamples <-  read.csv(fn)
mtsamples <- as_tibble(mtsamples)

```

## Question 1. How are specialties distributed?

```{r dist-of-specialties, cache = TRUE}
specialties <- mtsamples %>%
  count(medical_specialty)

specialties %>%
  arrange(desc(n)) %>%
  top_n(n, 15) %>%
  knitr::kable()
```

There are `r nrow(specialties)` specialties. Let's take a look at the distribution:

```{r, cache = TRUE}
#
#Here we plot straight from the original data frame, flipping it to make the axis look nicer.
ggplot(mtsamples, aes(x = medical_specialty)) +
  geom_histogram(stat = "count") +
  coord_flip()

#Here we plot from the limited data frame, reordering the specialty list based on the n value.
ggplot(specialties, aes(x = n, y = fct_reorder(medical_specialty, n))) +
  geom_col()

```

These are not overlapping, nor are they evenly distributed.

## Question 2. 

```{r token-transcript, cache = TRUE}
#Here we use the tidytext tool unnest_tokens to split the column "transcription" into individual words, labeling them "word". We then count each word and sort them. We then find the top 20 words. We then plot a bar chart of these 20 words, ordering the words based on the "n" assigned by the "count" function.
mtsamples %>%
  unnest_tokens(output = word, input = transcription) %>%
  count(word, sort = TRUE) %>%
  top_n(20) %>%
  ggplot(aes(x = n, y = fct_reorder(word, n))) +
    geom_col()

```

This makes sense but is not illuminating. These are boring words that everybody uses.

## Question 3: Remove the stop words and numbers.

```{r token-transcript-wo-stopwords, cache = TRUE}
#We use the tidytext tool "unnest_tokens" to split the column "transcription" into individual words, labeling them "word". We then count the words, and sort them. We then use the dplyr tool "anti_join" which returns all rows that aren't in the given data frame. We then use regex to remove numbers (grepl returns TRUE when a pattern is found, the pattern is any number). We then plot the top 20.

mtsamples %>%
  unnest_tokens(output = word, input = transcription) %>%
  count(word, sort = TRUE) %>%
  anti_join(stop_words, by = "word") %>%
  filter(!grepl(pattern = "^[0-9]+$", x = word)) %>%
  top_n(20) %>%
  ggplot(aes(x = n, y = fct_reorder(word, n))) +
    geom_col()
```

This is far more interesting. Clearly this is a medical text.

## Question 4. Tokenize into bi-grams and tri-grams.

```{r bi-grams, cache = TRUE}
# Instead of unnest_tokens, we now use unnest_ngrams on the "transcription" column. We assign it to 2 word combinations, and call them "bigrams". We count these bigrams, sort them, find the top 20, and plot them. We do not remove stop words.
mtsamples %>%
  unnest_ngrams(output = bigram, input = transcription, n = 2) %>%
  count(bigram, sort = TRUE) %>%
  top_n(20) %>%
  ggplot(aes(x = n, y = fct_reorder(bigram, n))) +
    geom_col()
```

Tri-grams:

```{r tri-grams, cache = TRUE}
#The same as above, but with 3 words in a row.
mtsamples %>%
  unnest_ngrams(output = trigram, input = transcription, n = 3) %>%
  count(trigram, sort = TRUE) %>%
  top_n(20) %>%
  ggplot(aes(x = n, y = fct_reorder(trigram, n))) +
    geom_col()
```

Some fun phrases have appeared.

## Question 5: Find words before and after a keyword.

```{r words-near-operation, cache = TRUE, warning = FALSE}
# We create a data frame called bigrams by calculating bigrams as we did before. We then use the tidyr function "separate" which breaks up our bigram based on spaces. We name the new variables "w1" and "w2". We then use the dyplr function "filter" to choose only the rows that either have "operation" assigned to w1 or w2.
bigrams <- mtsamples %>%
  unnest_ngrams(output = bigram, input = transcription, n = 2) %>%
  separate(bigram, into = c("w1", "w2"), sep = " ") %>%
  filter((w1 == "operation") | (w2 == "operation"))

# We take this data frame, which has w1 and w2 columns (one of which contains the word operation), and filter again for when w1 is "operation". We then use the dplyr function "select" to create a tibble of just w1 and w2. We then (dplyr) count the words in w2 and sort them on frequency. This finds all words that come after "operation".
bigrams %>%  
  filter(w1 == "operation") %>%
  select(w1, w2) %>%
  count(w2, sort = TRUE)

#The same as above, except now we find words before "operation". 
bigrams %>%  
  filter(w2 == "operation") %>%
  select(w1, w2) %>%
  count(w1, sort = TRUE)
```

Let's filter out stop words and numbers.

## Question 6. 

```{r words-near-operation-wo-stop, cache = TRUE}
# Same as above, except after we filter for w1 = "operation", we filter out any rows in which w2 either belongs to the list of stop words or is a number. We then count, sort, and display it.
bigrams %>%
  filter(w1 == "operation") %>%
  filter(!(w2 %in% stop_words$word) & !grepl("^[0-9]+$", w2)) %>%
  count(w2, sort = TRUE) %>%
  top_n(10) %>%
  knitr::kable(caption = "Words before 'operation'")

# Same as above, except for words coming before "operation". 
bigrams %>%
  filter(w2 == "operation") %>%
  filter(!(w1 %in% stop_words$word) & !grepl("^[0-9]+$", w2)) %>%
  count(w1, sort = TRUE) %>%
  top_n(10) %>%
  knitr::kable(caption = "Words after 'operation'")
```


## Question 6

```{r, cache = TRUE}
mtsamples %>%
  unnest_tokens(word, input = transcription) %>%
  group_by(medical_specialty) %>%
  count(word, sort = TRUE) %>%
  filter(!(word %in% stop_words$word) & !grepl("^[0-9]+$", word)) %>%
  top_n(5) %>%
  arrange(medical_specialty, n) %>%
  knitr::kable()

```





